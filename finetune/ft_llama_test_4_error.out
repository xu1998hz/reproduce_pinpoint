Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:30<01:30, 90.58s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:31<01:31, 91.26s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:30<01:30, 90.60s/it]Loading checkpoint shards:  50%|█████     | 1/2 [01:30<01:30, 90.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [02:02<00:00, 55.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [02:02<00:00, 61.16s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [02:03<00:00, 56.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [02:03<00:00, 61.86s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [02:02<00:00, 56.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [02:02<00:00, 61.21s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [02:03<00:00, 56.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [02:03<00:00, 61.66s/it]
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using /jet/home/gzhu/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /jet/home/gzhu/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /jet/home/gzhu/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Using /jet/home/gzhu/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /jet/home/gzhu/.cache/torch_extensions/py311_cu121/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Loading extension module cpu_adam...
wandb: Currently logged in as: guanglei-zhu (ian-team). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.0
wandb: Run data is saved locally in /ocean/projects/cis230075p/gzhu/reproduce_pinpoint/finetune/wandb/run-20240115_222745-ckye4gof
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandb
wandb: ⭐️ View project at https://wandb.ai/ian-team/huggingface
wandb: 🚀 View run at https://wandb.ai/ian-team/huggingface/runs/ckye4gof
  0%|          | 0/53 [00:00<?, ?it/s]  2%|▏         | 1/53 [01:48<1:34:15, 108.76s/it]                                                   2%|▏         | 1/53 [01:48<1:34:15, 108.76s/it]  4%|▍         | 2/53 [03:26<1:26:52, 102.21s/it]                                                   4%|▍         | 2/53 [03:26<1:26:52, 102.21s/it]  6%|▌         | 3/53 [05:04<1:23:37, 100.35s/it]                                                   6%|▌         | 3/53 [05:04<1:23:37, 100.35s/it]  8%|▊         | 4/53 [06:42<1:21:15, 99.50s/it]                                                   8%|▊         | 4/53 [06:42<1:21:15, 99.50s/it]  9%|▉         | 5/53 [08:19<1:18:41, 98.37s/it]                                                  9%|▉         | 5/53 [08:19<1:18:41, 98.37s/it] 11%|█▏        | 6/53 [09:54<1:16:13, 97.30s/it]                                                 11%|█▏        | 6/53 [09:54<1:16:13, 97.30s/it] 13%|█▎        | 7/53 [11:30<1:14:13, 96.81s/it]                                                 13%|█▎        | 7/53 [11:30<1:14:13, 96.81s/it]/ocean/projects/cis230075p/gzhu/miniconda3/envs/test/lib/python3.11/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201450/work/torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/ocean/projects/cis230075p/gzhu/miniconda3/envs/test/lib/python3.11/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201450/work/torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/ocean/projects/cis230075p/gzhu/miniconda3/envs/test/lib/python3.11/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201450/work/torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/ocean/projects/cis230075p/gzhu/miniconda3/envs/test/lib/python3.11/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201450/work/torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
 15%|█▌        | 8/53 [13:11<1:13:36, 98.15s/it]                                                 15%|█▌        | 8/53 [13:11<1:13:36, 98.15s/it] 17%|█▋        | 9/53 [14:50<1:12:12, 98.46s/it]                                                 17%|█▋        | 9/53 [14:50<1:12:12, 98.46s/it] 19%|█▉        | 10/53 [16:26<1:09:59, 97.66s/it]                                                  19%|█▉        | 10/53 [16:26<1:09:59, 97.66s/it] 21%|██        | 11/53 [18:02<1:08:07, 97.32s/it]                                                  21%|██        | 11/53 [18:02<1:08:07, 97.32s/it] 23%|██▎       | 12/53 [19:38<1:06:14, 96.94s/it]                                                  23%|██▎       | 12/53 [19:38<1:06:14, 96.94s/it] 25%|██▍       | 13/53 [21:15<1:04:34, 96.86s/it]                                                  25%|██▍       | 13/53 [21:15<1:04:34, 96.86s/it] 26%|██▋       | 14/53 [22:52<1:02:55, 96.81s/it]                                                  26%|██▋       | 14/53 [22:52<1:02:55, 96.81s/it] 28%|██▊       | 15/53 [24:29<1:01:24, 96.96s/it]                                                  28%|██▊       | 15/53 [24:29<1:01:24, 96.96s/it] 30%|███       | 16/53 [26:09<1:00:17, 97.77s/it]                                                  30%|███       | 16/53 [26:09<1:00:17, 97.77s/it] 32%|███▏      | 17/53 [27:45<58:25, 97.38s/it]                                                  32%|███▏      | 17/53 [27:45<58:25, 97.38s/it] 34%|███▍      | 18/53 [29:22<56:48, 97.38s/it]                                                34%|███▍      | 18/53 [29:22<56:48, 97.38s/it]slurmstepd: error: *** JOB 21691238 ON v006 CANCELLED AT 2024-01-15T22:57:28 ***
