Sun Jan 14 23:45:18 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A6000    Off  | 00000000:01:00.0 Off |                  Off |
| 30%   37C    P8    31W / 300W |      1MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A6000    Off  | 00000000:25:00.0 Off |                  Off |
| 30%   34C    P8    37W / 300W |      1MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA RTX A6000    Off  | 00000000:41:00.0 Off |                  Off |
| 30%   48C    P2    77W / 300W |  30035MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA RTX A6000    Off  | 00000000:61:00.0 Off |                  Off |
| 30%   31C    P8    27W / 300W |  11803MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  NVIDIA RTX A6000    Off  | 00000000:81:00.0 Off |                  Off |
| 30%   33C    P8    29W / 300W |      1MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  NVIDIA RTX A6000    Off  | 00000000:A1:00.0 Off |                  Off |
| 30%   29C    P8    32W / 300W |      1MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  NVIDIA RTX A6000    Off  | 00000000:C1:00.0 Off |                  Off |
| 30%   49C    P2    80W / 300W |  15055MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  NVIDIA RTX A6000    Off  | 00000000:E1:00.0 Off |                  Off |
| 30%   31C    P8    30W / 300W |      1MiB / 49140MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    3   N/A  N/A    158166      C   python                            262MiB |
+-----------------------------------------------------------------------------+
[2024-01-14 23:45:22,111] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-14 23:45:24,787] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-01-14 23:45:24,788] [INFO] [runner.py:571:main] cmd = /home/guangleizhu/miniconda3/envs/torch2.1/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgNCwgNSwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None finetune_llama.py
[2024-01-14 23:45:27,783] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-14 23:45:30,143] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 4, 5, 7]}
[2024-01-14 23:45:30,144] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=5, node_rank=0
[2024-01-14 23:45:30,144] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4]})
[2024-01-14 23:45:30,144] [INFO] [launch.py:163:main] dist_world_size=5
[2024-01-14 23:45:30,144] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,4,5,7
[2024-01-14 23:45:36,007] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-14 23:45:36,053] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-14 23:45:36,058] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-14 23:45:36,068] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-01-14 23:45:36,077] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Dataset({
    features: ['output', 'input'],
    num_rows: 6847
}) Dataset({
    features: ['output', 'input'],
    num_rows: 5
})
Start loading in tokenizers
Dataset({
    features: ['output', 'input'],
    num_rows: 6847
}) Dataset({
    features: ['output', 'input'],
    num_rows: 5
})
Start loading in tokenizers
Dataset({
    features: ['output', 'input'],
    num_rows: 6847
}) Dataset({
    features: ['output', 'input'],
    num_rows: 5
})
Start loading in tokenizers
Dataset({
    features: ['output', 'input'],
    num_rows: 6847
}) Dataset({
    features: ['output', 'input'],
    num_rows: 5
})
Start loading in tokenizers
Dataset({
    features: ['output', 'input'],
    num_rows: 6847
}) Dataset({
    features: ['output', 'input'],
    num_rows: 5
})
Start loading in tokenizers
Start loading in model meta-llama/Llama-2-7b-chat-hf
Start loading in model meta-llama/Llama-2-7b-chat-hf
Start loading in model meta-llama/Llama-2-7b-chat-hf
Start loading in model meta-llama/Llama-2-7b-chat-hf
Start loading in model meta-llama/Llama-2-7b-chat-hf
add pad token and resize embedding: True
add pad token and resize embedding: True
add pad token and resize embedding: True
add pad token and resize embedding: True
add pad token and resize embedding: True
Loaded in model and tokenizers
Start making data module
Loaded in model and tokenizers
Start making data module
Loaded in model and tokenizers
Start making data module
Loaded in model and tokenizers
Start making data module
Loaded in model and tokenizers
Start making data module
[2024-01-14 23:46:58,947] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-14 23:47:01,123] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-14 23:47:01,123] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-01-14 23:47:01,543] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-14 23:47:01,910] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-01-14 23:47:02,256] [INFO] [comm.py:637:init_distributed] cdb=None
Start the trainer
Start the trainer
Start the trainer
Start the trainer
Start the trainer
[2024-01-14 23:47:47,219] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
[2024-01-14 23:47:47,243] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
[2024-01-14 23:47:47,258] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
[2024-01-14 23:47:47,264] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
[2024-01-14 23:47:47,266] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 2.40950083732605 seconds
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 2.4580636024475098 seconds
Time to load cpu_adam op: 2.5401082038879395 seconds
Time to load cpu_adam op: 2.5458309650421143 seconds
Time to load cpu_adam op: 2.546367645263672 seconds
Parameter Offload: Total persistent parameters: 266240 in 65 params
{'loss': 12.8177, 'learning_rate': 1e-05, 'epoch': 0.02}
{'loss': 12.88, 'learning_rate': 1e-05, 'epoch': 0.04}
{'loss': 12.9155, 'learning_rate': 1e-05, 'epoch': 0.05}
{'loss': 12.9047, 'learning_rate': 1e-05, 'epoch': 0.07}
{'loss': 12.7993, 'learning_rate': 1e-05, 'epoch': 0.09}
{'loss': 12.8614, 'learning_rate': 1e-05, 'epoch': 0.11}
{'loss': 12.9023, 'learning_rate': 1e-05, 'epoch': 0.12}
{'loss': 12.8244, 'learning_rate': 1e-05, 'epoch': 0.14}
{'loss': 12.8563, 'learning_rate': 1e-05, 'epoch': 0.16}
{'loss': 12.8528, 'learning_rate': 1e-05, 'epoch': 0.18}
{'loss': 12.8093, 'learning_rate': 1e-05, 'epoch': 0.19}
{'loss': 12.8565, 'learning_rate': 1e-05, 'epoch': 0.21}
{'loss': 12.8511, 'learning_rate': 1e-05, 'epoch': 0.23}
{'loss': 12.7682, 'learning_rate': 1e-05, 'epoch': 0.25}
{'loss': 12.8836, 'learning_rate': 1e-05, 'epoch': 0.26}
{'loss': 12.8484, 'learning_rate': 1e-05, 'epoch': 0.28}
{'loss': 12.8352, 'learning_rate': 1e-05, 'epoch': 0.3}
{'loss': 12.7717, 'learning_rate': 1e-05, 'epoch': 0.32}
{'loss': 12.8667, 'learning_rate': 1e-05, 'epoch': 0.33}
{'loss': 12.802, 'learning_rate': 1e-05, 'epoch': 0.35}
{'loss': 12.853, 'learning_rate': 1e-05, 'epoch': 0.37}
{'loss': 12.8439, 'learning_rate': 1e-05, 'epoch': 0.39}
{'loss': 12.8511, 'learning_rate': 9.992407582166582e-06, 'epoch': 0.4}
{'loss': 10.8127, 'learning_rate': 9.969653386589749e-06, 'epoch': 0.42}
{'loss': 14.6894, 'learning_rate': 9.969653386589749e-06, 'epoch': 0.44}
{'loss': 13.519, 'learning_rate': 9.969653386589749e-06, 'epoch': 0.46}
{'loss': 15.9483, 'learning_rate': 9.969653386589749e-06, 'epoch': 0.47}
{'loss': 14.861, 'learning_rate': 9.969653386589749e-06, 'epoch': 0.49}
{'loss': 15.3356, 'learning_rate': 9.969653386589749e-06, 'epoch': 0.51}
{'loss': 18.2284, 'learning_rate': 9.969653386589749e-06, 'epoch': 0.53}
{'loss': 12.1342, 'learning_rate': 9.969653386589749e-06, 'epoch': 0.54}
{'loss': 19.7609, 'learning_rate': 9.969653386589749e-06, 'epoch': 0.56}
{'loss': 20.087, 'learning_rate': 9.969653386589749e-06, 'epoch': 0.58}
{'loss': 11.8466, 'learning_rate': 9.969653386589749e-06, 'epoch': 0.6}
{'loss': 24.0047, 'learning_rate': 9.969653386589749e-06, 'epoch': 0.61}
[2024-01-15 01:51:55,044] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 182644
[2024-01-15 01:52:12,347] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 182645
[2024-01-15 01:52:12,387] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 182646
[2024-01-15 01:52:12,419] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 182647
[2024-01-15 01:52:12,419] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 182648
[2024-01-15 01:52:12,449] [ERROR] [launch.py:321:sigkill_handler] ['/home/guangleizhu/miniconda3/envs/torch2.1/bin/python', '-u', 'finetune_llama.py', '--local_rank=4'] exits with return code = 1
